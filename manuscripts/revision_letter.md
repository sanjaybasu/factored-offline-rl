# Revision Letter

## Offline Reinforcement Learning for Care Management: Addressing Sparse Rewards, Temporal Dependencies, and Fairness in Medicaid Populations

**Submission ID:** f059fc56-e523-43ee-8c99-24732c79a3c5

---

Dear Dr. Dunn and Reviewers,

We thank the editor and reviewers for their thorough and constructive feedback. We have addressed each comment through a combination of new analyses, additional supplementary materials, and targeted manuscript revisions. Below we provide a point-by-point response. Changes in the revised manuscript are indicated in **bold**. New Supplementary Tables S12--S16 have been added.

---

## Reviewer 1

### Comment R1.1: Internal comparison design

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| A central concern is the comparison design, which is almost entirely internal to the proposed framework. While the ablation studies (DR vs WIS, LSTM vs feedforward, fairness-constrained vs unconstrained) are informative, the manuscript does not compare against alternative frameworks applied to the same data. This makes it difficult to disentangle whether the observed stability and non-inferiority arise from the proposed evaluation methodology itself or from a tightly coupled set of design choices optimized within a closed framework. | We agree that comparing against an external offline RL baseline strengthens the evidence. We trained a Conservative Q-Learning (CQL) policy---a widely-used offline RL algorithm that penalizes Q-values for out-of-distribution actions---on the same data, state representation, and reward function. We evaluated the CQL policy using our doubly robust estimator, ensuring an apples-to-apples comparison. Results (Supplementary Table S12) show that CQL achieves a DR policy value of --0.0811 (95% CI: --0.0903 to --0.0719), which is also non-inferior to the behavioral policy but modestly inferior to the AWR-based LSTM policy (--0.0736). The CQL policy achieved a comparable effective sample size (41.8%) and demographic parity gap (1.1 pp), confirming that the doubly robust evaluation framework produces stable estimates regardless of the upstream RL algorithm. We acknowledge in the Discussion that a broader comparison across additional offline RL methods (e.g., IQL, BCQ) would further strengthen conclusions, and note this as a direction for future work. | Methods: new subsection "Comparison to Alternative Offline RL Algorithms"; Results: new paragraph in "Variance Inflation and Doubly Robust Evaluation"; Discussion: updated "Comparison to Prior Work" subsection; Supplement: new Table S12 |

### Comment R1.2: Reward function under-specified

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| In addition, the design of the reward function is insufficiently specified and justified. While the manuscript describes a multi-component reward combining acute care outcomes, engagement signals, and cost, the explicit functional form and numerical weights are not reported. The manuscript states that weights were selected through validation to optimize correlation with long-term outcomes while maintaining clinical face validity, but this data-dependent weight selection raises concerns. Clarification on how the reward was specified, why particular weights were chosen, and how sensitive the main conclusions are to these choices would substantially strengthen the credibility and reproducibility of the findings. | We have substantially expanded the reward function specification. First, we corrected the manuscript to accurately reflect the implementation: the three components are primary acute care outcomes, engagement signals, and intermediate clinical milestones (not cost, as previously stated). The component weights are $w_p=1.0$, $w_e=0.3$, $w_i=0.5$. We now report the full functional form with all sub-component numerical values: the engagement component awards +0.2 for successful patient contact, +0.1 for attempted contact, --0.1 for appointment no-shows, and +0.15 for appointment attendance; the intermediate milestone component awards +0.3 for primary care visits, +0.2 for medication fills, +0.1 for lab completions, +0.2 for specialist visits, and --0.2 for missed appointments. Second, we report a comprehensive reward weight sensitivity analysis (Supplementary Table S14) showing policy performance across a 4 x 4 grid of engagement and intermediate weights (16 configurations). The analysis includes correlation with long-term outcomes, Jensen-Shannon divergence of action distributions, and the percentage of decisions where the recommended action changes. Results show that the primary outcome signal dominates policy behavior, with less than 8% of actions changing across the full range of weight configurations, and DR policy values varying by less than 3%. Third, we added a sentence noting that the weight selection procedure used systematic grid search on held-out validation data, not post-hoc optimization on the test set. | Methods: expanded "Multi-Component Reward Shaping" subsection with full functional form and sub-component values; Abstract: corrected "cost" to "intermediate clinical milestones"; Supplement: corrected Table S3 (cost row replaced with intermediate milestones); new Table S14 (reward weight sensitivity) |

### Comment R1.3: Figure 1 font size

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| Lastly, Figure 1 is informative and helpful for understanding the overall framework, but its readability would be improved by increasing the font size. | We have regenerated Figure 1 with all text elements at a minimum of 12-point font size to improve readability. | Figure 1 |

---

## Reviewer 2

### Comment R2.1: Fairness and use of race/ethnicity mediators

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| The decision to exclude race and ethnicity from policy inputs is reasonable and defensible from a deployment and ethical perspective. However, the framework implicitly assumes that observed mediators (e.g., socioeconomic indicators, utilization history, and social determinants of health) sufficiently capture structural disadvantage associated with race and ethnicity. This assumption is strong and not directly tested. Without explicit causal analysis, residual race-related pathways may persist through imperfect proxies. Additional analyses clarifying whether these mediators adequately block race-related causal paths would strengthen the fairness claims. | We agree this is an important concern. We added a regression-based mediation analysis (Supplementary Table S16) estimating the proportion of the race-outcome association explained by observed social determinants of health (SDoH) variables. Using logistic regression, we estimated the total association between race/ethnicity and acute care outcomes (Model 1: outcome ~ race + age + sex), then the direct association after adjusting for SDoH mediators including housing stability, food security, transportation access, insurance continuity, and area deprivation index (Model 2: outcome ~ race + age + sex + SDoH). Results show that 47--62% of the race-outcome association is explained by included SDoH mediators, depending on the racial/ethnic group. This suggests that observed mediators capture a substantial but incomplete portion of structural disadvantage. We added a new Limitations paragraph acknowledging that residual race-related pathways may persist through unmeasured mediators (e.g., provider implicit bias, neighborhood-level environmental exposures, historical disenrollment patterns) and recommending formal causal mediation frameworks for future work. | Discussion: new paragraph in Limitations; Supplement: new Table S16 |

### Comment R2.2: LSTM window length sensitivity

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| The use of a 4-week history window for the LSTM is clinically plausible and aligns with typical care management cycles. However, this choice is not shown to be optimal or robust. Sensitivity analyses comparing alternative window lengths (e.g., 2, 6, or 8 weeks) would strengthen the argument that the learned policy does not critically depend on this specific temporal design choice and would help clarify the bias--variance tradeoff in temporal modeling. | We conducted sensitivity analyses with window lengths of 2, 4, 6, and 8 weeks (Supplementary Table S13). The 4-week window achieved the best validation loss (0.457) and DR policy value (--0.074). The 2-week window showed modestly worse performance (DR value: --0.082), consistent with insufficient temporal context. The 6- and 8-week windows showed marginal degradation (DR values: --0.077 and --0.079, respectively), likely reflecting the bias-variance tradeoff: longer windows capture more history but increase sequence modeling difficulty and reduce the number of valid training sequences. Effective sample sizes were comparable across all window lengths (41--45%), confirming that the evaluation methodology is robust to this design choice. These results support the 4-week window as a clinically and statistically appropriate choice, while demonstrating that conclusions are not critically dependent on this specific temporal design decision. | Results: new sentence referencing Table S13; Discussion: brief mention in Limitations (temporal resolution paragraph); Supplement: new Table S13 |

### Comment R2.3: Reward shaping may mis-align policy; engagement as SES proxy

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| The proposed reward shaping strategy substantially increases signal density and shows strong correlation with long-term outcomes. However, correlation alone is insufficient to guarantee policy alignment. A shaped reward can predict outcomes well yet still induce a different optimal policy by over-incentivizing intermediate signals (e.g., engagement) that are only indirectly related to the primary clinical objective. Furthermore, the engagement reward (e.g., patient contact, appointment attendance) may also hiddenly represent one's social-economic status and capability, which varied greatly in different demographic subgroups in reality. While integrating engagement indicators via reward function can increase non-zero reward occurrences, it's concerning that it will depolarize patients with less access to healthcare resources but with higher clinical demands. Sensitivity analyses over reward component weights and examinations of policy-induced action shifts would help assess whether reward shaping alters the decision objective in unintended ways. | We address this concern in three ways. First, our reward weight sensitivity analysis (Supplementary Table S14) now includes policy action shift analysis, reporting Jensen-Shannon divergence and the fraction of actions that change under each of 16 weight configurations relative to the base case. Results show that fewer than 8% of actions change across the full range, and that the primary outcome component accounts for 68% of reward variance, confirming that the primary clinical objective dominates policy behavior. Second, we added a Discussion paragraph addressing the concern that engagement rewards may serve as a socioeconomic status (SES) proxy. We acknowledge that patients with greater healthcare access may achieve higher engagement rewards, potentially amplifying existing advantages. However, we note that the demographic parity constraint during training explicitly penalizes differential intervention rates across racial/ethnic groups, mitigating this concern. The proportional parity analysis (Table S15) further confirms that high-resource interventions are allocated proportionally to clinical need across demographic subgroups. Third, we note that the sensitivity analysis shows that even when engagement weight is reduced to 0.1 (one-third of the base case), the policy's action distribution changes minimally (JS divergence < 0.02), suggesting the policy is not critically dependent on engagement signals. | Discussion: new paragraph on engagement as SES proxy; Supplement: Table S14 (action shift analysis columns); Table S15 (proportional parity confirms need-proportional allocation) |

### Comment R2.4: Fairness definition -- proportional parity

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| The definition of fairness in this study should be more clearly introduced in the abstract. For the fairness analysis, the authors used "demographic parity" as the evaluation metrics. While this is a popular choice, "demographic parity" also has some limitations, e.g., it does not take true class into consideration and only depends on the model predictions. In this scenario, another metric named "proportional parity" is more suitable because it allocates resources or sends interventions proportional to the demands or needs. It would be more helpful to have some analysis and results about the fairness evaluation using "proportional parity" and keep "demographic parity" results as baseline performance. | We agree that proportional parity is a valuable complementary fairness metric. We now report proportional parity results alongside demographic parity (Supplementary Table S15). We define proportional parity as the ratio of each group's intervention rate to its baseline clinical need (measured as the group's acute care utilization rate). A ratio of 1.0 indicates perfectly need-proportional allocation. Results show that proportional parity ratios range from 0.92 to 1.08 across racial/ethnic groups under the fairness-constrained policy, indicating near-proportional allocation relative to need. We also clarified the fairness definitions in the Abstract and Methods sections. Demographic parity remains the training-time constraint (as it is differentiable and amenable to gradient-based optimization), while proportional parity is reported as a complementary auditing metric that accounts for differential baseline need across groups. | Abstract: added "proportional parity" to fairness clause; Methods: expanded "Fairness-Constrained Training" subsection with proportional parity definition; Results: brief mention in "Policy Performance and Fairness"; Supplement: new Table S15 |

### Comment R2.5: Comparison with other offline RL algorithms (CQL)

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| The entire paper focuses on comparison with standard weighted importance sampling, is there any comparison with other offline RL models (e.g., conservative Q-learning)? | See response to R1.1 above. We trained a Conservative Q-Learning (CQL) baseline and evaluated it using the same doubly robust estimator (Supplementary Table S12). | Same as R1.1 |

### Comment R2.6: Zero-inflated models

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| Given the extremely rare primary outcome (0.6% weekly emergency department visits and 0.09% for hospitalizations), does the author ever consider adopting the count or zero-inflated models as part of the data analysis? Providing a paragraph of justification would be helpful, even if the count / zero-inflated models are ultimately not appropriate for this setting. | We added a Discussion paragraph addressing this question. Zero-inflated models (e.g., zero-inflated Poisson, hurdle models) are designed for modeling zero-heavy count distributions in prediction or regression settings. In the reinforcement learning framework, however, the agent does not directly model the outcome distribution; rather, it learns a policy to maximize cumulative reward. The sparsity challenge in RL is not that the outcome distribution is zero-inflated per se, but that sparse binary rewards provide insufficient gradient signal for policy optimization. Our multi-component reward shaping addresses this by converting near-binary outcomes into continuous, information-rich learning signals through engagement and intermediate milestone components, increasing non-zero reward occurrences from 0.60% to 32.1%. This approach preserves policy optimality properties under potential-based reward shaping theory (Ng et al. 1999). We note that the Q-function model used in doubly robust evaluation does implicitly handle the zero-inflation in outcomes through its regression structure (XGBoost), which naturally accommodates skewed and zero-heavy distributions without requiring explicit zero-inflated parameterization. | Discussion: new paragraph after "Principal Findings" subsection |

### Comment R2.7: Training data size (50% split)

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| In the "Statistical Analysis" section, the authors used 50% of data for training. Using only 50% of the data as a training set is generally considered low; typically, 70% to 80% of the data is used for training. Some explanation or justification for this choice is needed. Otherwise, it may raise concerns about "cherry-picking", where only reporting the data-splitting strategy that yields the highest performance, even if the reported one seems atypical. | We added justification for the 50/25/25 split in the Methods section. Three considerations motivated this choice. First, the central methodological contribution of this work is the doubly robust evaluation framework, which requires a large held-out test set (25%) for unbiased policy assessment---the evaluation methodology is itself the primary contribution, and underpowering it would undermine the paper's central claims. Second, the 25% validation set supports 100-trial Bayesian hyperparameter optimization with median pruning, which requires sufficient data for stable early-stopping decisions. Third, with 9,998,139 total observations, the 50% training set comprises approximately 5 million observations, yielding a parameter-to-observation ratio exceeding 145:1 for the LSTM architecture (34,337 parameters), which substantially exceeds standard recommendations for neural network training. We also note that the 50/25/25 split is consistent with standard practice in offline RL evaluation studies where the evaluation component is methodologically central (e.g., Thomas & Brunskill, 2016). | Methods: expanded "Statistical Analysis" subsection |

### Comment R2.8: Overly strong language

| Reviewer Comment (Verbatim) | How We Addressed It | Location of Revision |
|---|---|---|
| Try to avoid phrases like "resolves the fundamental evaluation crisis" when, in reality, the proposed learning policy was trained to be only "non-inferior" to observed behavioral policy. | We have systematically softened the language throughout the manuscript. Specific changes include: "resolves the fundamental evaluation crisis" changed to "addresses the evaluation challenge"; "completely fails" changed to "produces unreliable estimates"; "statistically unusable" and "statistically meaningless" changed to "statistically unreliable"; "are not optional enhancements but necessary foundations" changed to "provide essential variance reduction for reliable performance assessment." We reviewed the full manuscript (Abstract, Introduction, Discussion, Conclusions) to ensure all claims are measured and evidence-based, appropriately reflecting the non-inferiority finding rather than implying superiority. | Abstract (multiple instances); Introduction (paragraphs 1--2); Discussion (paragraphs 1--3, "Principal Findings"); Conclusions |

---

We believe these revisions comprehensively address all reviewer and editor comments. We are grateful for the opportunity to strengthen the manuscript and look forward to the reviewers' assessment of our revisions.

Sincerely,

Sanjay Basu, MD, PhD, on behalf of all authors
